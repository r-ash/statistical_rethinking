---
title: "Chapter 4"
output: html_document
---
```{r}
library(rethinking)
```
# Easy

Model definition
$$
y_{i} \sim Normal(\mu,\sigma)
$$
$$
\mu \sim Normal(0, 10)
$$
$$
\sigma \sim Exponential(1)
$$

1. Line 1 is the likelihood

2. 2 parameters in the posterior distribution, `\sigma` and `\mu`

3. Write down the appropriate form of Bayes' theorem that includes the proper likelihood and priors

$$
Pr(\mu,\sigma | y) = \frac{\prod_{i}{Normal(y_{i}|\mu,\sigma)Normal(\mu|0,10)Exponential(\sigma|1)}}{\int{\int{\prod_{i}{Normal(y_{i}|\mu,\sigma)Normal(\mu|0,10)Exponential(\sigma|1)}}d\mu d\sigma}}
$$

4. 

$$
y_{i} \sim Normal(\mu,\sigma)
$$
$$
\mu_{i} = \alpha + \beta x_{i}
$$
$$
\alpha \sim Normal(0, 10)
$$
$$
\beta \sim Normal(0,1)
$$
$$
\sigma \sim Exponential(2)
$$
Line 2 is the linear model.

5. There are 3 parameters? $$\alpha$$, $$\beta$$ and $$\sigma$$

## Medium

1. Simulate observed `y` values from the prior (not the posterior) for the definition

$$
y_{i} \sim Normal(\mu,\sigma)
$$
$$
\mu \sim Normal(0, 10)
$$
$$
\sigma \sim Exponential(1)
$$

```{r}
curve(dnorm(x, 0, 10), from = -40, to = 40)
```

```{r}
curve(dexp(x, 1), from = 0, to = 10)
```

```{r}
sample_mu <- rnorm(1e4, 0, 10)
sample_sigma <- rexp(1e4, 1)
prior_y <- rnorm(1e4, sample_mu, sample_sigma)
dens(prior_y)
```

2. Translate the model into a `quap` formula

```{r}
flist <- alist(
  y ~ dnorm(mu, sigma),
  mu ~ dnorm(0, 10),
  sigma ~ dexp(1)
)
```

3. Translate the `quap` formula into a mathematical model definition

```{r}
ex <- alist(
  y ~ dnorm( mu , sigma ),
  mu <- a + b*x,
  a ~ dnorm( 0 , 10 ),
  b ~ dunif( 0,1),
  sigma ~ dexp( 1 )
)
```

$$
y_{i} \sim Normal(\mu, \sigma)
$$
$$
\mu_{i} = \alpha + \beta x_{i}
$$
$$
\alpha \sim Normal(0, 10)
$$
$$
\beta \sim Uniform(0, 1)
$$
$$
\sigma \sim Exponential(1)
$$

4. A sample of students is measured for height each year for 3 years. After the 3rd year, you want to fit a linear regression predicting height using year as a predictor. Write down the mathematical model definition for this regression, using any variable names and priors you choose. Be prepared to defend your choice of priors.

$$
h_{i} \sim Normal(\mu, \sigma)
$$
$$
\mu = \alpha + \beta y
$$
$$
\alpha \sim Normal(178, 20)
$$
$$
\beta \sim Exponential(0.25)
$$
$$
\sigma \sim Uniform(0, 50)
$$
Going for exponential beta as I expect that height in students will increase year on year, so want to ignore <0. I think ideal world could have a prior that wouldn't have it's mode at 0 as they are more likely to grow but thought this would be close enough. Use rate 0.25 as made the curve look "not too flat"
```{r}
curve(dexp(x, rate = 0.25), from = 0, to = 10)
```

5. Doesn't change as I tried to factor that into part 4

6. We have alpha is normally disributed with $$\sigma^{2} = 20$$, if within a single age never vary by more than 64cm, we can use this to change $$\alpha$$. In normal, 95% of the data is within 2sds of the mean. So we could put 95% bounds at 64cm. So `(a + 2sd) - (a - 2sd) = 64` => `sd = 16`

```{r}
curve(dnorm(x, mean = 178, sd = 16), from = 130, to = 230)
```
Or would we revise to 64cm to cover 99.7% by using 3 sds from the mean? Would give tighter prior

```{r}
curve(dnorm(x, mean = 178, sd = 16), from = 130, to = 230)
```

7. Refit model m4.3, but omit the mean weight `xbar`. Compare the new posterior to the original model, look at covariance among parameters. Compare posterior predictions

```{r}
library(rethinking)

data(Howell1)

d <- Howell1
d2 <- d[d$age >= 18, ]
```


```{r}
plot(NULL, 
     xlim = range(d2$weight), 
     ylim = c(-100, 400),
     xlab = "weight", 
     ylab = "height")

abline(h = 0, lty = 2)

abline(h = 272, lty = 1, lwd = 0.5)

mtext("b ~ dnorm(0,10)")

N <- 100
a <- rnorm(N, 178, 20)
b <- runif(N, 0, 1)

for (i in 1:N) {
  curve(a[i] + b[i]*(x),
        from = min(d2$weight), 
        to = max(d2$weight), 
        add = TRUE,
        col = col.alpha("black", 0.2))
}
```
```{r}
fit <- quap(
  alist(
    height ~ dnorm(mu, sigma),
    mu <- a + b * weight,
    a ~ dnorm(130, 10),
    b ~ dunif(0, 2),
    sigma ~ dunif(0, 50)
  ),
  data = d2
)
```

```{r}
xbar <- mean(d2$weight)
m4.3 <- quap(
  alist(
    height ~ dnorm( mu , sigma ) ,
    mu <- a + b*( weight - xbar ) ,
    a ~ dnorm( 178 , 20 ) ,
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
    ),
  data = d2)
```

Compare new models posterior to that of the original model

```{r}
precis(fit)
```

```{r}
precis(m4.3)
```
The mean of a much lower in the model without average weight. That makes sense as before when x = 0, y was the average eight but now it is the height of a weight 0 person which is kind of meaningless. The standard deviation of this is much higher too. I feel like that makes intuitive sense as the value there is outside the range of the data so we are going to be more uncertain about its value.

Look at covariance among the parameters

```{r}
round(vcov(fit), 3)
```

```{r}
round(vcov(m4.3), 3)
```

```{r}
cov2cor(vcov(fit))
```

In the fit without the mean weight there is higher covariance between a and b. Which we can see on the graph below.

```{r}
pairs(fit)
```

```{r}
pairs(m4.3)
```

What do we read from this...

Compare the posterior predictions of both models

```{r}
plot( height ~ weight , data = d2 , col = rangi2 )
post <- extract.samples( fit )
a_map <- mean(post$a)
b_map <- mean(post$b)
curve( a_map + b_map*(x) , add = TRUE )
```

```{r}
par(mfrow = c(1,2))
weight.seq <- seq( from = 25 , to = 70 , by = 1 )
mu <- link(fit , data = data.frame(weight = weight.seq) )
plot( height ~ weight , d2 , type = "n" )
for (i in 1:100) {
  points( weight.seq , mu[i,] , pch = 16 , col = col.alpha(rangi2,0.1) )
}
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI , prob = 0.89 )

mu4.3 <- link(m4.3 , data = data.frame(weight = weight.seq) )
plot( height ~ weight , d2 , type = "n" )
for (i in 1:100) {
  points( weight.seq , mu4.3[i,] , pch = 16 , col = col.alpha(rangi2,0.1) )
}
mu4.3.mean <- apply( mu4.3 , 2 , mean )
mu4.3.PI <- apply( mu4.3 , 2 , PI , prob = 0.89 )
```


```{r}
par(mfrow = c(1,2))
plot( height ~ weight , data = d2 , col = col.alpha(rangi2,0.5) )
lines( weight.seq , mu.mean )
shade( mu.PI , weight.seq )

plot( height ~ weight , data = d2 , col = col.alpha(rangi2,0.5) )
lines( weight.seq , mu4.3.mean )
shade( mu4.3.PI , weight.seq )
```

## Hard questions

1. Weights listed below were recorded in the !Kung census, but heights were not recorded for these individuals. Provide predicted heights and 89% intervals for each of these individuals. Fill in the table below, using model-based predictions.

```{r}
weights <- c(46.95, 43.72, 64.78, 32.59, 54.63)
```

```{r}
library(rethinking)
data(Howell1)
d <- Howell1
d2 <- d[ d$age >= 18 , ]

xbar <- mean(d2$weight)
m4.3 <- quap(
  alist(
    height ~ dnorm( mu , sigma ),
    mu <- a + b*( weight - xbar ),
    a ~ dnorm( 178 , 20 ),
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
), data = d2)
```

```{r}
mu <- link(m4.3, data = data.frame(weight = weights))
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)
out <- data.frame(weight = weights, 
                  "expected height" = mu.mean, 
                  t(mu.PI))
```

2. Select all the rows in `Howell1` data with ages below 18 years of age.

```{r}
u18 <- d[ d$age < 18 , ]
```

a. Fit a linear regression using `quap`. Present and interpret the estimates. For every 10 units of increase in weight, how much taller does the model predict a child gets?

```{r}
xbar <- mean(u18$weight)
mu18 <- quap(
  alist(
    height ~ dnorm( mu , sigma ),
    mu <- a + b*( weight - xbar ),
    a ~ dnorm( 100 , 40 ),
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
), data = u18)
```

```{r}
plot(height ~ weight, data = u18, col = rangi2)
post <- extract.samples(mu18)
a_map <- mean(post$a)
b_map <- mean(post$b)
curve(a_map + b_map * (x - xbar), add = TRUE)
```

```{r}
precis(mu18)
```

For every 10 unit increase in weight, there is a `10 * b_map` increase in height

```{r}
10 * b_map
```

b. Plot raw data, height on vertical and weight on horizontal. Superimpose the map regression line and 89% interval for the mean. Also superimpose the 89% interval for predicted heights.

```{r}
post <- extract.samples(mu18)
a_map <- mean(post$a)
b_map <- mean(post$b)
model_func <- function(x) {
  a_map + b_map * (x - xbar)
}
mu <- link(mu18)
mu.mean <- apply(mu, 2, mean)
mu.PI <- apply(mu, 2, PI, prob = 0.89)

sim_heights <- sim(mu18)
height_pi <- apply(sim_heights, 2, PI, prob = 0.89)

ggplot2::ggplot(u18, ggplot2::aes(weight, height)) +
  ggplot2::geom_ribbon(ggplot2::aes(ymin = mu.PI[1, ], ymax = mu.PI[2, ]), 
                       fill = "grey50", alpha = 0.5) +
  ggplot2::geom_ribbon(ggplot2::aes(ymin = height_pi[1, ], ymax = height_pi[2, ]),
                       fill = "grey80", alpha = 0.5) + 
  ggplot2::geom_point(shape = 21, colour = "blue") +
  ggplot2::geom_function(fun = model_func) + 
  ggplot2::theme_bw()

## How does extract.samples relate to link?
## I don't really trust this uncertainty here.. Looks really narrow compared to the real data, why is that?

# plot(height ~ weight, data = u18, col = rangi2)
# curve(a_map + b_map * (x - xbar), add = TRUE)
```

c. What about the model fit concerns you? Describe the kind of assumptions you would change, if any to improve the model .

Can clearly see the data has curve and fitting a straight line through it seems to have some issues. The model is doing a bad job at both tail ends. I would incorporate another term into this to capture the curvature... though what that looks like? Some x^2 term?

3. Attempt to model age ages, but modelling logarithm of body weight as scaling with height.

```{r}
full_model <- quap(
  alist(
    height ~ dnorm( mu , sigma ),
    mu <- a + b * log(weight),
    a ~ dnorm( 120 , 60 ),
    b ~ dlnorm( 0 , 1 ) ,
    sigma ~ dunif( 0 , 50 )
), data = d)
```

```{r}
post <- extract.samples(full_model)
a_map <- mean(post$a)
b_map <- mean(post$b)
model_func <- function(x) {
  a_map + b_map * log(x)
}

mu <- link(full_model)
mu.mean <- apply(mu, 2, mean)
mu.HPDI <- apply(mu, 2, HPDI, prob = 0.97)

sim_heights <- sim(full_model)
height_pi <- apply(sim_heights, 2, PI, prob = 0.89)

ggplot2::ggplot(d, ggplot2::aes(weight, height)) +
  ggplot2::geom_ribbon(ggplot2::aes(ymin = mu.HPDI[1, ], ymax = mu.HPDI[2, ]), 
                       fill = "grey50", alpha = 0.5) +
  ggplot2::geom_ribbon(ggplot2::aes(ymin = height_pi[1, ], ymax = height_pi[2, ]),
                       fill = "grey80", alpha = 0.5) + 
  ggplot2::geom_point(shape = 21, colour = "blue") +
  ggplot2::geom_function(fun = model_func) + 
  ggplot2::theme_bw()
```
